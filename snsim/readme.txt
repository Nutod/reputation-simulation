Instructions for running the Reputation simulation

1.  Modify json config file for your run
		"conserv_javaswitch_debug.json" is an example of a config file
			It contains only one batch run
			It contains our best so far parameters
		config files have three sections: test, batch and parameters.  
			Test tells what unittests and ranges apply to this scenario,
			Batch tells what parameters in the parameter section to run a cartesian product on, and what to call the outfiles
			Parameters contains all the parameters of a single run
		All the parameters you want to modify for a simple run are in the parameters section
			The switch that tells if you you are running java or python is at ['parameters']['use_java']
			The name of the output directory is at ['parameters']['output_path']
			The port you are running on is at ['parameters']['port']
			The random number seed is at ['parameters']['seed']
			All the reputation system parameters are at ['parameters']['reputation_parameters']
		
2.  Call ReputationSim.py <configfile name>
		create the output directory manually first, as the program does not create directories

3.  Output files are in the output path and are coded by the switched out parameters designated in the configfile batch section
		params file reprints the config file generated by the batch for each run
		transactions files have all the current transactions
		boolean files tell if an agent is good or bad
		user files tell the how good the agent is
		market volume report reports on market volumes of trade between types of agents
		rank history prints out the ranks of all agents in each row, with -1 meaning no ranking
		
4.  To get metrics on the run, run unittests , which are soft unittests for anlysis purposes
		make sure the config file contains the tests that you want run on each parameter combination
		change the config file name in every test in the reputation/test folder to the current config file
			 sed -i 's/conserv_new_thres33.json/conserv_javaswitch_debug.json/g' *Tests.py
		metric results will appear in the output file
			error_log.txt contains all the metrics, in appended timestamp mode to record metric changes over time
			For worksheet analysis, individual test results appear in tsv files
				These tsv files can be read into a jupyter notebook, that generates a composite tsv for analysis in a worksheet
				reputation_results-conserv_new_unrestricted_mem.ipynb is one example
			